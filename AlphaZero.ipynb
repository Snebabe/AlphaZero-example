{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687111dc-c984-43fd-9821-45515141d498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# ! python.exe -m pip install --upgrade pip\n",
    "# ! pip install numpy\n",
    "# ! pip install torch\n",
    "# ! pip install tqdm\n",
    "# ! pip install ipywidgets --upgrade\n",
    "# ! pip install jupyter\n",
    "# ! pip install --upgrade notebook jupyterlab ipywidgets\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0091477c-0530-49eb-9ce5-f861807bb771",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.column_count = 7\n",
    "        self.action_size = self.column_count\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return np.zeros((self.row_count, self.column_count))\n",
    "\n",
    "    def get_next_state(self, state, action: int, player):\n",
    "        for row in reversed(range(self.row_count)):\n",
    "            if state[row, action] == 0:\n",
    "                state[row, action] = player\n",
    "                return state\n",
    "\n",
    "        # should not happen\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action): \n",
    "        if action == None: return False\n",
    "        # Get player\n",
    "        for row in range(self.row_count):\n",
    "            if state[row][action] != 0:\n",
    "                player = state[row][action]\n",
    "                break\n",
    "        for row in range(self.row_count):\n",
    "            for col in range(self.column_count):\n",
    "                if self.check_direction(row, col, 1, 0, player, state) or \\\n",
    "                   self.check_direction(row, col, 0, 1, player, state) or \\\n",
    "                   self.check_direction(row, col, 1, 1, player, state) or \\\n",
    "                   self.check_direction(row, col, 1, -1, player, state):\n",
    "                    return True\n",
    "\n",
    "    def check_direction(self, row, col, drow, dcol, player, state):\n",
    "        count = 0\n",
    "        for _ in range(4):\n",
    "            if 0 <= row < self.row_count and 0 <= col < self.column_count and state[row][col] == player:\n",
    "                count += 1\n",
    "                if count >= 4:\n",
    "                    return True\n",
    "            else:\n",
    "                break\n",
    "            row += drow\n",
    "            col += dcol\n",
    "        return False\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        # Encode it for the AI\n",
    "        # Look at the architecture notes in docs\n",
    "        # We want three input states, one for each player and one with empty cells\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        return encoded_state\n",
    "    \n",
    "    def print_state(self, state):\n",
    "        symbols = {0: \"-\", 1: \"X\", -1: \"O\"}\n",
    "        display = \"\\n\".join(\" \".join(symbols[cell] for cell in row) for row in state)\n",
    "        print(display)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30bbc4bf-a805-4871-be18-7cccf370f59b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Start by creating our game, which AlphaZero will play.\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.column_count = 3\n",
    "        # action size is the amount of possible actions (moves)\n",
    "        self.action_size = self.row_count * self.column_count\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        # new board = board full of zeros\n",
    "        return np.zeros((self.column_count, self.row_count))\n",
    "\n",
    "    def get_next_state(self, state, action: int, player):\n",
    "        \"\"\"\n",
    "        state = board (2D array)\n",
    "        action = 0-8 which cell to place move\n",
    "        player = player\n",
    "        \"\"\"\n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        state[row, column] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        # state.reshape() reshapes the matrix, in our case from 3x3 into something else\n",
    "        # the -1 in state.reshape(-1) tells np to reshape automatically\n",
    "        # in our case our 3x3 will become 1x9, the output is the new np matrix\n",
    "        # looking like this: ([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        # by doing == 0 we get an output array like this:\n",
    "        # np.array([True, True, True, True, True, True, True, True, True])\n",
    "        # the last code, astype, tells np to convert the values into unsigned integers, making the array:\n",
    "        # np.array([1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        if action == None:\n",
    "            return False\n",
    "            \n",
    "        row = action // self.column_count\n",
    "        column = action % self.column_count\n",
    "        player = state[row, column]\n",
    "\n",
    "        return (\n",
    "            # Checks for rows, assume player is an integer\n",
    "            np.sum(state[row, :]) == player * self.column_count\n",
    "            or np.sum(state[:, column]) == player * self.row_count\n",
    "            # Checks for diagonals, \n",
    "            or np.sum(np.diag(state)) == player * self.row_count\n",
    "            or np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "    \n",
    "    def get_encoded_state(self, state):\n",
    "        # Encode it for the AI\n",
    "        # Look at the architecture notes in docs\n",
    "        # We want three input states, one for each player and one with empty cells\n",
    "        encoded_state = np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        return encoded_state\n",
    "\n",
    "    def print_state(self, state):\n",
    "        symbols = {0: \"-\", 1: \"X\", -1: \"O\"}\n",
    "        display = \"\\n\".join(\" \".join(symbols[cell] for cell in row) for row in state)\n",
    "        print(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93b2b0d4-8404-4ad1-a62b-1a2a8e67eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal MCTS\n",
    "\n",
    "# Here, the node will never know of the player\n",
    "# Instead, the rest of the code is programmed so that the state of the board\n",
    "# will always be from the nodes perspective (i.e. flipped, or inverted might be a better word)\n",
    "# For the node, the player is always player 1.\n",
    "# Logic should be easier, and also valid for 1 player games\n",
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        # probability by policy vector from parent, used in expansion \n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        # Not needed with CNN, we expand all everytime\n",
    "        # self.expandable_moves = game.get_valid_moves(state)\n",
    "\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        # OLD, with CNN all will be expanded at once\n",
    "        # return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        if child.visit_count == 0:\n",
    "            q_value = 0\n",
    "        else:\n",
    "            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        return q_value + self.args['C'] * child.prior * (math.sqrt(self.visit_count)/(child.visit_count + 1))\n",
    "\n",
    "        # OLD\n",
    "        # What the child thinks of itself, not the parent,\n",
    "        # Remember, the child is the opponent of the parent\n",
    "        # # That is why we take 1 - UCB\n",
    "        # q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n",
    "        # return q_value + self.args['C'] * math.sqrt(math.log(self.visit_count)/child.visit_count)\n",
    "\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "        # (OLD)\n",
    "        # Choose one random expansion option\n",
    "        # action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n",
    "        # self.expandable_moves[action] = 0\n",
    "\n",
    "        # child_state = self.state.copy()\n",
    "        # child_state = self.game.get_next_state(child_state, action, 1)\n",
    "        # child_state = self.game.change_perspective(child_state, player=-1)\n",
    "\n",
    "        # child = Node(self.game, self.args, child_state, self, action)\n",
    "        # self.children.append(child)\n",
    "        # return child\n",
    "    \n",
    "    # Unused when CNN\n",
    "    #\n",
    "    # def simulate(self):\n",
    "    #     value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n",
    "    #     value = self.game.get_opponent_value(value)\n",
    "\n",
    "    #     if is_terminal: return value\n",
    "    #     # all other cases: rollout!\n",
    "    #     rollout_state = self.state.copy()\n",
    "    #     rollout_player = 1\n",
    "    #     while True:\n",
    "    #         valid_moves = self.game.get_valid_moves(rollout_state)\n",
    "    #         action = np.random.choice(np.where(valid_moves == 1)[0])\n",
    "    #         rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n",
    "    #         value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n",
    "    #         if is_terminal:\n",
    "    #             if rollout_player == -1:\n",
    "    #                 value = self.game.get_opponent_value(value)\n",
    "    #             return value\n",
    "\n",
    "    #         rollout_player = self.game.get_opponent(rollout_player)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        # flip player\n",
    "        value = self.game.get_opponent_value(value)\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "        \n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    # decorator as we don't want to store gradients\n",
    "    # don't know why\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        # define root \n",
    "        root = Node(self.game, self.args, state)\n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            # selection\n",
    "            while node.is_fully_expanded():\n",
    "                node = node.select()\n",
    "\n",
    "            # The action taken here will be from the opponent,\n",
    "            # Because of that, we take the negative value\n",
    "            # That is, the value seen from our node\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0)\n",
    "                )\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                # mask out invalid moves, \n",
    "                policy *= valid_moves\n",
    "                # normalize, \n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                # .item() on tensor with 1 float = you get the float\n",
    "                # this value replaces the simulation part\n",
    "                value = value.item()\n",
    "\n",
    "                # expansion\n",
    "                node.expand(policy)\n",
    "            # backpropagation\n",
    "            node.backpropagate(value)\n",
    "        \n",
    "        # return visit_counts\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "        # Turn into probabilities, i.e. we want values in [0, 1].\n",
    "        # In this MCTS, the node with the most amount of visits \n",
    "        # is deemed the most promising move\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cded231",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "  def __init__(self, model, optimizer, game, args):\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.game = game\n",
    "    self.args = args\n",
    "    self.mcts = MCTS(game, args, model)\n",
    "\n",
    "  def selfPlay(self):\n",
    "    memory = []\n",
    "    player = 1\n",
    "    state = self.game.get_initial_state()\n",
    "\n",
    "    while True: \n",
    "      # remember, we want to search from player 1 perspective\n",
    "      neutral_state = self.game.change_perspective(state, player)\n",
    "      action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "      memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "      action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "\n",
    "      state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "      value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "      if is_terminal:\n",
    "        returnMemory = []\n",
    "        for hist_neutral_state, hist_action_probs, hist_player in memory:\n",
    "          hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n",
    "          returnMemory.append((\n",
    "            self.game.get_encoded_state(hist_neutral_state),\n",
    "            hist_action_probs,\n",
    "            hist_outcome\n",
    "          ))\n",
    "        return returnMemory\n",
    "    \n",
    "      player = self.game.get_opponent(player)\n",
    "\n",
    "  def train(self, memory):\n",
    "      random.shuffle(memory)\n",
    "      for batchIdx in range(0, len(memory), self.args['batch_size']):\n",
    "          sample = memory[batchIdx:min(len(memory)-1, batchIdx + self.args['batch_size'])]\n",
    "          # Transpose sample \n",
    "          state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "          state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n",
    "\n",
    "          state = torch.tensor(state, dtype=torch.float32)\n",
    "          policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "          value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "\n",
    "          out_policy, out_value = self.model(state)\n",
    "\n",
    "          policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "          value_loss = F.mse_loss(out_value, value_targets)\n",
    "          loss = policy_loss + value_loss\n",
    "\n",
    "          self.optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()          \n",
    "\n",
    "  def learn(self):\n",
    "    for iteration in range(self.args['num_iterations']):\n",
    "      memory = []\n",
    "\n",
    "      self.model.eval()\n",
    "      # trange = range() but with progress bars\n",
    "      for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n",
    "        memory += self.selfPlay()\n",
    "\n",
    "      self.model.train()\n",
    "      for epoch in trange(self.args['num_epochs']):\n",
    "        self.train(memory)\n",
    "\n",
    "      torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cb48af7-f573-429b-ae49-ccf13778f6ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resBlocks, num_hidden):\n",
    "        super().__init__()\n",
    "        self.startBlock = nn.Sequential(\n",
    "            # we can use kernel = 3 because we use padding = 1, \n",
    "            # so image will still be 3x3 (if you want to look at it like an image)\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            # increase training speed\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            # turn negatives to 0s, faster and safer to train\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backBone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "        )\n",
    "\n",
    "        self.policyHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            # this is the policy vector\n",
    "            # it should output the size of possible moves\n",
    "            # I think, why not just use len(game.get_valid_moves(state)), idk\n",
    "            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.valueHead = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.column_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.startBlock(x)\n",
    "        for resBlock in self.backBone:\n",
    "            x = resBlock(x)\n",
    "        policy = self.policyHead(x)\n",
    "        value = self.valueHead(x)\n",
    "        return policy, value\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b782d478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952d8aaa4b394f73b1fff1979620aa96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaec4a12eb4441ca5005fe6911faf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fe217a73754cbc886c0c8134c4d211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fefb61405d4d71b9cc6e26d074956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6cf1e734b514e44beacfa9a83419c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f71bb009924814a90c00f167a11061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dbd576b92f4138be4f23546a23fae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5b941d0fd04009a926c45a49e5da82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca40833c0264cd09c7fcdad2fdfce89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 23\u001b[0m\n\u001b[0;32m     13\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m80\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m alphaZero \u001b[38;5;241m=\u001b[39m AlphaZero(model, optimizer, game, args)\n\u001b[1;32m---> 23\u001b[0m \u001b[43malphaZero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 70\u001b[0m, in \u001b[0;36mAlphaZero.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# trange = range() but with progress bars\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selfPlay_iteration \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_selfPlay_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 70\u001b[0m   memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfPlay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[47], line 17\u001b[0m, in \u001b[0;36mAlphaZero.selfPlay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[0;32m     15\u001b[0m   \u001b[38;5;66;03m# remember, we want to search from player 1 perspective\u001b[39;00m\n\u001b[0;32m     16\u001b[0m   neutral_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mchange_perspective(state, player)\n\u001b[1;32m---> 17\u001b[0m   action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneutral_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m   memory\u001b[38;5;241m.\u001b[39mappend((neutral_state, action_probs, player))\n\u001b[0;32m     21\u001b[0m   action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39maction_size, p\u001b[38;5;241m=\u001b[39maction_probs)\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[17], line 137\u001b[0m, in \u001b[0;36mMCTS.search\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    134\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_opponent_value(value)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_terminal:\n\u001b[1;32m--> 137\u001b[0m     policy, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoded_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     policy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(policy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    141\u001b[0m     valid_moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_valid_moves(node\u001b[38;5;241m.\u001b[39mstate)\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resBlock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackBone:\n\u001b[0;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m resBlock(x)\n\u001b[1;32m---> 42\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicyHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalueHead(x)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy, value\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Dokument\\GitHub\\AlphaZero-example\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# AlphaZero testing\n",
    "\n",
    "game = ConnectFour()\n",
    "\n",
    "model = ResNet(game, 4, 64)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('model_2.pt'))\n",
    "except FileNotFoundError:\n",
    "    print(\"No trained model found, training from scratch.\")\n",
    "    \n",
    "args = {\n",
    "  'C': 2,\n",
    "  'num_searches': 80,\n",
    "  'num_iterations': 8,\n",
    "  'num_selfPlay_iterations': 500,\n",
    "  'num_epochs': 4,\n",
    "  'batch_size': 64\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, game, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d707d0e5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01498559769243002\n",
      "[0.07369338 0.08111365 0.28695878 0.19949225 0.15006292 0.11499906\n",
      " 0.09367993]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0.]]\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 0., 0., 1., 1., 1.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 0., 0., 0., 0.]]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CNN testing\n",
    "# ! pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tictactoe = ConnectFour()\n",
    "\n",
    "state = tictactoe.get_initial_state()\n",
    "state = tictactoe.get_next_state(state, 2, 1)\n",
    "state = tictactoe.get_next_state(state, 3, -1)\n",
    "\n",
    "encoded_state = tictactoe.get_encoded_state(state)\n",
    "\n",
    "# no clue vad tensor, array av arrays?\n",
    "# typ för att representera en nod och deras inkommande weights\n",
    "# Because one state, and not a batch, we need to unsqueeze (ingen aning)\n",
    "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "model = ResNet(tictactoe, 4, 64)\n",
    "# Nu är den tränad, vamos\n",
    "model.load_state_dict(torch.load('model_2.pt'))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "# ingen aning\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "print(policy)\n",
    "print(state)\n",
    "print(tensor_state)\n",
    "\n",
    "plt.bar(range(tictactoe.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eaedb9-7579-4ccb-b35e-2e461c22a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - -\n",
      "- - - - - - -\n",
      "- - - - - - -\n",
      "- - - - - - -\n",
      "- - - - - - -\n",
      "- - - - - - -\n",
      "valid moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "### CNN and MCTS\n",
    "\n",
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 100\n",
    "}\n",
    "\n",
    "model = ResNet(game, 4, 64)\n",
    "model.load_state_dict(torch.load('model_3.pt'))\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    game.print_state(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print(\"valid moves:\", [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f\"{player}:\"))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print(\"action not valid\")\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        policy = mcts.search(state)\n",
    "        # encoded_state = game.get_encoded_state(neutral_state)\n",
    "        # tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "        # policy, _ = model(tensor_state)\n",
    "        # policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "        plt.bar(range(game.action_size), policy)\n",
    "        plt.show()\n",
    "        action = np.argmax(policy)\n",
    "    \n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        game.print_state(state)\n",
    "        if value == 1:\n",
    "            print(player, \"won\")\n",
    "        else: \n",
    "            print(\"draw\")\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f389d9-004d-4ef4-9bde-2b25f0541e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python AlphaZero Example venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
